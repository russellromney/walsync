---
title: Multi-Database Scalability
description: The core advantage of Walsync over Litestream
---

## The Problem: Resource Bloat

**Litestream:** One database = one process

```
┌─────────────┐  ┌─────────────┐  ┌─────────────┐
│  tenant1.db │  │  tenant2.db │  │  tenant3.db │
└──────┬──────┘  └──────┬──────┘  └──────┬──────┘
       │                │                │
   Litestream      Litestream      Litestream
   Process 1       Process 2       Process 3
   (50 MB)         (50 MB)         (50 MB)
       │                │                │
       └────────────────┼────────────────┘
                        │
                    S3 (Tigris)
```

**Cost:** 3 databases × 50MB/process = **150MB**

With 10 databases: **500MB overhead** just for replication!

## The Solution: Single Process

**Walsync:** One process for N databases

```
┌─────────────┐  ┌─────────────┐  ┌─────────────┐
│  tenant1.db │  │  tenant2.db │  │  tenant3.db │
└──────┬──────┘  └──────┬──────┘  └──────┬──────┘
       │                │                │
       └────────────────┼────────────────┘
                        │
                   Walsync
                  (10 MB total)
                        │
                    S3 (Tigris)
```

**Cost:** 3 databases × 1 process/3 = **10MB**

With 10 databases: **10MB overhead** - shared process!

## Scale Comparison

| Scenario | Litestream | Walsync | Memory Saved |
|----------|-----------|---------|--------------|
| 1 DB | 50 MB | 10 MB | 40 MB |
| 5 DB | 250 MB | 10 MB | **240 MB** |
| 10 DB | 500 MB | 10 MB | **490 MB** |
| 50 DB | 2500 MB | 10 MB | **2490 MB** |
| 100 DB | 5000 MB | 10 MB | **4990 MB** |

## How It Works

### Concurrent Snapshots

Walsync handles multiple databases simultaneously:

```rust
walsync watch \
  /data/tenant1.db \
  /data/tenant2.db \
  /data/tenant3.db \
  -b s3://backups/app
```

Behind the scenes:
1. **File Watcher** - Monitors all database files
2. **WAL Tracker** - Independent offset per database
3. **S3 Client** - Shared connection pool
4. **Upload Queue** - Batched uploads

### Per-Database Isolation

Each database tracked independently:
- Separate WAL frame tracking
- Independent snapshot scheduling
- Isolated restore operations
- No cross-database interference

### Resource Sharing

One process efficiently uses:
- Single tokio runtime (async executor)
- Shared S3 connection pool
- Shared file descriptor cache
- Single memory allocator

## Real-World Example

### Tenement/Slum Multi-Tenant Setup

100 tenant databases, each generating ~1MB/day of WAL:

**With Litestream:**
```bash
# Need 100 separate processes
for tenant in {1..100}; do
  litestream replicate /data/$tenant.db s3://backups &
done
# 100 × 50MB = 5000 MB overhead
# Plus process management overhead
```

**With Walsync:**
```bash
# Single process
walsync watch /data/*.db -b s3://backups

# 10 MB total overhead
# No per-database management needed
```

**Savings:** 4990 MB memory + simpler operations

## Performance Characteristics

### Startup Time

| Scenario | Litestream | Walsync |
|----------|-----------|---------|
| 1 DB | ~100ms | ~10ms |
| 10 DB | ~1000ms | ~10ms |
| 100 DB | ~10s | ~10ms |

Walsync startup is constant regardless of database count.

### Memory Curve

```
5000 MB │                      Litestream (50MB × N)
        │              •
2500 MB │         •
        │    •
 500 MB │•
        │  Walsync (10MB constant)
  100 MB │──────────────────────────
        │
          1    5    10   50   100
                  Databases
```

### Upload Performance

With shared S3 connection pool:
- ✅ Reduced connection overhead
- ✅ Better HTTP pipelining
- ✅ Improved throughput per database
- ✅ Same latency (single process still single-threaded WAL reading)

## Use Cases

### Perfect for Walsync
- ✅ **Multi-tenant apps** - Many databases
- ✅ **Microservices** - Database per service
- ✅ **Edge deployments** - Limited resources
- ✅ **Kubernetes** - Simpler pod management
- ✅ **VPS/Dedicated** - Resource-constrained

### Still Fine with Litestream
- ✅ **Single database** - No advantage to Walsync
- ✅ **Go-only stacks** - Operational familiarity
- ✅ **Established workflows** - Already using Litestream

## Architecture Details

### File Watcher

Uses platform-specific efficient watchers:
- **Linux:** inotify (kernel-level)
- **macOS:** FSEvents (kernel-level)
- **Windows:** ReadDirectoryChangesW (kernel-level)

Single watcher thread monitors all databases.

### WAL Tracking

Independent state per database:
```
Database 1: frame_offset=1024, timestamp=20240112T120000Z
Database 2: frame_offset=2048, timestamp=20240112T120005Z
Database 3: frame_offset=512,  timestamp=20240112T115955Z
```

Each restored independently via `walsync restore db1`, etc.

### Snapshot Scheduling

Configurable per-database intervals:
```bash
# All databases: hourly snapshots
walsync watch /data/*.db --snapshot-interval 3600

# Can customize per database with configuration file
```

---

**Bottom line:** Walsync efficiently manages many databases with a single process, saving massive amounts of memory compared to per-process solutions like Litestream.
